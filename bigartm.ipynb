{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('./papers.csv')\n",
    "all_texts = df.paper_text\n",
    "all_texts[111]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as esw\n",
    "\n",
    "# import SyllableTokenizer() method from nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import string\n",
    "\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "ps = PorterStemmer()\n",
    "cachedStopWords = [\"fig\", \"figure\", \"et\", \"al\", \"table\", \"data\", \"analysis\", \"analyze\", \"study\", \"method\", \"result\", \"conclusion\", \"author\", \"find\", \"found\", \"show\", \"perform\", \"demonstrate\", \"evaluate\", \"discuss\", \"google\", \"scholar\", \"pubmed\", \"web\", \"science\", \"crossref\", \"supplementary\", \"(fig.)\", \"(figure\", \"fig.\", \"al.\", \"did\", \"thus,\", \"â€¦\", \"\" \"\", \"interestingly\", \"and/or\", \"author\"] + list(esw)\n",
    "\n",
    "def lemmatize_article(sentence):\n",
    "    sentence = word_tokenize(sentence)\n",
    "    res = ''\n",
    "    for word in sentence:\n",
    "        word = lemmatizer.lemmatize(word)\n",
    "        res += word + ' '\n",
    "    return res\n",
    "    \n",
    "def remove_stop_words(sentence):\n",
    "    return ' '.join([word for word in sentence.split() if word not in cachedStopWords])\n",
    "    \n",
    "def remove_short(sentence):\n",
    "    return ' '.join([word for word in sentence.split() if len(word) >= 3])\n",
    "    \n",
    "def remove_digits(sentence):\n",
    "    return ' '.join([i for i in sentence.split() if not i.isdigit()])\n",
    "    \n",
    "def preprocess(all_texts):\n",
    "    all_texts = list(map(lambda x: x.lower(), all_texts))\n",
    "    all_texts = list(map(lambda x: x.translate(str.maketrans('', '', string.punctuation)), all_texts))\n",
    "    all_texts = list(map(lambda x: lemmatize_article(x), all_texts))\n",
    "    all_texts = list(map(lambda x: x.strip(), all_texts))\n",
    "    all_texts = list(map(lambda x: remove_stop_words(x), all_texts))\n",
    "    all_texts = list(map(lambda x: remove_short(x), all_texts))\n",
    "    all_texts = list(map(lambda x: remove_digits(x), all_texts))\n",
    "    return all_texts\n",
    "    \n",
    "    \n",
    "all_texts = preprocess(all_texts)\n",
    "all_texts[111]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(text, n ):\n",
    "    n_grams = ngrams(word_tokenize(text), n)\n",
    "    return [' '.join(grams) for grams in n_grams]\n",
    "\n",
    "bigrams = []\n",
    "for article in all_texts:\n",
    "    bigrams += list(map(lambda x: x[0], list(filter(lambda x: x[1] >= 5, Counter(get_ngrams(article, 2)).most_common()))))\n",
    "    \n",
    "bigrams = list(filter(lambda x: 'package' not in x and 'document' not in x, bigrams))\n",
    "bigrams = list(map(lambda x: x[0], (list(filter(lambda x: x[1] >= 5, Counter(bigrams).most_common())))))\n",
    "\n",
    "print(len(bigrams))\n",
    "print(bigrams[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "n_wd_bigrams = np.empty((len(bigrams), len(all_texts)))\n",
    "\n",
    "for i in range(len(bigrams)):\n",
    "    for j in range(len(all_texts)):\n",
    "        n_wd_bigrams[i][j] = all_texts[j].count(bigrams[i])\n",
    "        \n",
    "cv = CountVectorizer(max_features = features, stop_words='english')\n",
    "n_wd = np.array(cv.fit_transform(all_texts).todense()).T\n",
    "vocabulary = cv.get_feature_names()\n",
    "\n",
    "n_wd = np.concatenate((n_wd, n_wd_bigrams))\n",
    "vocabulary += bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install bigartm\n",
    "\n",
    "import artm\n",
    "\n",
    "model_artm = artm.ARTM(topic_names = topic_names, cache_theta=True, scores=[artm.PerplexityScore(name='PerplexityScore', dictionary=dictionary), \n",
    "                                                                    artm.SparsityPhiScore(name='SparsityPhiScore'),\n",
    "                                                                    artm.SparsityThetaScore(name='SparsityThetaScore'),\n",
    "                                                                    artm.TopicKernelScore(name='TopicKernelScore',probability_mass_threshold=0.3), \n",
    "                                                                    artm.TopTokensScore(name='TopTokensScore', num_tokens=8)],\n",
    "                       regularizers=[artm.SmoothSparseThetaRegularizer(name='SparseTheta', tau=-0.4),\n",
    "                                     artm.DecorrelatorPhiRegularizer(name='DecorrelatorPhi', tau=2.5e+5)])\n",
    "                                     \n",
    "model_artm.num_document_passes = 4\n",
    "model_artm.initialize(dictionary)\n",
    "model_artm.fit_offline(batch_vectorizer=bv, num_collection_passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def print_measures(model_artm):\n",
    "    print('Sparsity Phi ARTM:{}'.format(model_artm.score_tracker['SparsityPhiScore'].last_value))\n",
    "    print('Sparsity Theta ARTM:{}'.format(model_artm.score_tracker['SparsityThetaScore'].last_value))\n",
    "    print('Perplexity ARTM: {}'.format(model_artm.score_tracker['PerplexityScore'].last_value))\n",
    "    \n",
    "    ig, axs = plt.subplots(1, 3, figsize = (30, 5))\n",
    "    \n",
    "    for idx, score, y_label in zip(range(3), ['PerplexityScore', 'SparsityPhiScore', 'SparsityThetaScore'], ['ARTM perplexity', 'ARTM Phi sparsity', 'ARTM Theta sparsity']):\n",
    "        axs[idx].plot(range(model_artm.num_phi_updates), model_artm.score_tracker[score].value, 'r--', linewidth=2)\n",
    "        axs[idx].set_xlabel('Iterations count')\n",
    "        axs[idx].set_ylabel(y_label)\n",
    "        axs[idx].grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_articles_on_theme(dataset, topic, num_topics):\n",
    "    theta = np.array(model_artm.get_theta('topic_{}'.format(topic)).iloc[0]).theta[theta <= 0.05] = 0\n",
    "    idx = np.nonzero(theta)[0]\n",
    "    articles = zip(idx, theta[idx])\n",
    "    articles = sorted(articles, key = lambda x: x[1], reverse = True)\n",
    "    articles = [x[0] for x in articles]\n",
    "    return dataset.iloc[articles].PaperText[:num_topics]\n",
    "    \n",
    "get_articles_on_theme(df, 8, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_names = ['topic_{}'.format(i) for i in range(50)]\n",
    "\n",
    "model_artm1 = artm.ARTM(topic_names=topic_names,cache_theta=True, scores=[artm.PerplexityScore(name='PerplexityScore',                                dictionary=dictionary), artm.SparsityPhiScore(name='SparsityPhiScore'),\n",
    "                                                                  artm.SparsityThetaScore(name='SparsityThetaScore'), \n",
    "                                                                  artm.TopicKernelScore(name='TopicKernelScore',probability_mass_threshold=0.3),\n",
    "                                                                  artm.TopTokensScore(name='TopTokensScore',num_tokens=12)],\n",
    "                        regularizers=[artm.SmoothSparseThetaRegularizer(name='SparseTheta', tau=-0.4),\n",
    "                        artm.SmoothSparsePhiRegularizer(name='SparsePhi',tau=-0.25),\n",
    "                        artm.DecorrelatorPhiRegularizer(name='DecorrelatorPhi', tau=2.5e+5)], seed=243) #seed is required for heirarchy \n",
    "                        \n",
    "model_artm1.num_document_passes = 4\n",
    "model_artm1.set_parent_model(parent_model = model_artm, parent_model_weight = 0.75)\n",
    "model_artm1.initialize(dictionary)\n",
    "\n",
    "model_artm1.fit_offline(batch_vectorizer=bv, num_collection_passes=12)\n",
    "\n",
    "subt = pd.DataFrame(model_artm1.get_parent_psi())\n",
    "subt.columns = ['topic_{}'.format(i) for i in range(10)]\n",
    "subt.index = ['subtopic_{}'.format(i) for i in range(50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtopics_wrt_topic(topic_number, matrix_dist):\n",
    "   return matrix_dist.iloc[:, topic_number].sort_values(ascending = False)[:5]\n",
    "subtopics_wrt_topic(0, subt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doutorado",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
